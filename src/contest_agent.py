# src/contest_agent.py

# Importing required libraries for message handling and data validation
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from pydantic import BaseModel
from src.create_llm_message import create_llm_message, create_llm_msg
from src.book_appointment import handle_appointment_request, book_appointment
from datetime import datetime
from typing import Optional
from src.send_email import send_email
import markdown2
from src.prompt_store import get_prompt

# Data model for structuring the LLM's response
class ContestDecision(BaseModel):
    nextsteps: str
    decision: str
    timeslot: Optional[str] = None
    email: Optional[str] = None
    name: Optional[str] = None

# When ContestAgent object is created, it's initialized with a model. 
# The main entry point is the contest_agent method. You can see workflow.add_node for contest_agent node in graph.py

class ContestAgent:
    
    def __init__(self, model):
        """
        Initialize the ContestAgent with a ChatOpenAI model.
        
        :param model: An instance of the ChatOpenAI model used for generating responses.
        """
        self.model = model


    def generate_contest_response(self, messageHistory: list[BaseMessage]) -> str:
        """
        Generate a response for contest-related queries using the ChatOpenAI model.
        
        :param user_query: The original query from the user.
        :return: A string response generated by the language model.
        """
        # Get contest prompt from prompt_store.py
        contest_prompt = get_prompt("contest")

        # Create a well-formatted message for LLM by passing the contest_prompt above to create_llm_msg
        llm_messages = create_llm_msg(contest_prompt, messageHistory)

        # Invoke the model with the well-formatted prompt, including SystemMessage, HumanMessage, and AIMessage
        llm_response = self.model.invoke(llm_messages)
        
        full_response = llm_response.content
        
        return full_response

    def contest_agent(self, state: dict) -> dict:
        """
        Process user's contest-related questions and return appropriate responses.
        
        :param state: Dictionary containing conversation state and user's message
        :return: Dictionary containing:
            - lnode: Name of the current node ("contest_agent")
            - responseToUser: Contest info, URL, or next steps based on the decision
            - category: Type of response ("contest")
        """
        # Generate a response based on the user's initial message
        # Get AI's decision and recommended next steps
        full_response = self.generate_contest_response(state['message_history'])
        #llm_response.decision = llm_response.decision.replace("[", "").replace("]", "") #this line is for Groq LLM because it adds square brackets
        

        # Return the updated state with the generated response and the category set to 'contest'
        return {
            "lnode": "contest_agent", 
            "responseToUser": full_response,
            "category": "contest"
        }
